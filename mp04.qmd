---
title: "Mini Project 4"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r setup, include=FALSE}
library(sf)
library(httr2)
library(rvest)
library(stringr)
library(janitor)
library(dplyr)
library(knitr)
```

## ðŸ§© US County Shapes

I focused on acquiring data for the ***2020 U.S. Presidential Election*** by scraping county-level election results from Wikipedia. Using the rvest and httr2 libraries in R, I collected the relevant election data for each state. The data was cleaned by removing unnecessary columns, converting character values to numeric, and renaming ambiguous columns. This ensured that the dataset contained only relevant columns, such as the total votes for Joe Biden and Donald Trump, while also addressing inconsistencies in the column names. The cleaned dataset laid the foundation for the state-level analysis conducted in the subsequent tasks.

```{r}
#| label: Task 1
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: Show Code

# Directory setup
dir.create("data/mp04", recursive = TRUE, showWarnings = FALSE)

# URL for detailed county shapefiles
url <- "https://www2.census.gov/geo/tiger/GENZ2022/shp/cb_2022_us_county_500k.zip"
zip_path <- "data/mp04/county_shapefiles.zip"
shapefile_dir <- "data/mp04/county_shapes"

# Download only if not present
if (!file.exists(zip_path)) {
  download.file(url, destfile = zip_path, mode = "wb")
}

# Unzip shapefile
if (!dir.exists(shapefile_dir)) {
  unzip(zip_path, exdir = shapefile_dir)
}

# Read shapefile
county_shapes <- st_read(shapefile_dir)

# Preview (first 6 rows, selected columns)
library(dplyr)
library(knitr)

county_shapes_clean <- county_shapes |>
  select(STATEFP, COUNTYFP, NAME, geometry) |>
  head(6)

kable(county_shapes_clean)
```

## ðŸ§© 2024 US Presidential Election Results

Using the cleaning function developed earlier, I extracted and processed county-level election results for each state. This involved ensuring that the necessary vote columns were available and that the data was correctly formatted for analysis. After cleaning the data, I combined the results into a single dataset, making it easier to compute state-level summaries. This step was essential for preparing the dataset for deeper analysis, including vote totals and candidate comparisons across different states.

```{r}
#| label: Task 2
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: Show Code

# Function to scrape and clean the 2024 election results for a specific state
scrape_clean_state_results_2024 <- function(state_name, show_table = TRUE) {
  
  # Prepare the state name for use in the URL
  safe_state <- gsub(" ", "_", state_name)
  url <- paste0("https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_", safe_state)
  html_path <- file.path("data/mp04/html", paste0(safe_state, "_2024.html"))
  
  # Ensure the directory exists and download the HTML file if not already cached
  dir.create("data/mp04/html", recursive = TRUE, showWarnings = FALSE)
  
  if (!file.exists(html_path)) {
    resp <- request(url) |> req_perform()
    writeBin(resp_body_raw(resp), html_path)
  }
  
  # Read and extract tables from the HTML
  page <- read_html(html_path)
  tables <- page |> html_elements("table") |> html_table(fill = TRUE)
  
  # Find the table that contains county-level data
  county_table <- NULL
  for (tbl in tables) {
    if (any(str_detect(names(tbl), regex("County|Parish|Borough", ignore_case = TRUE)))) {
      county_table <- tbl
      break
    }
  }

  # Handle case where no county-level data was found
  if (is.null(county_table)) {
    warning(paste("No county-level results found for", state_name))
    return(NULL)
  }
  
  # Clean column names and standardize them
  county_table <- county_table |> clean_names()
  names(county_table) <- tolower(names(county_table))

  # Select only relevant columns (county, Trump votes, Biden votes, total votes, and turnout)
  relevant_cols <- county_table |> 
    select(matches("county|biden|trump|total|votes|turnout")) |> 
    mutate(state = state_name)

  # Return either the first few rows or the full cleaned table
  if (show_table) {
    return(kable(head(relevant_cols, 10), caption = paste("2024 Results for", state_name)))
  } else {
    return(relevant_cols)
  }
}

# Example usage for Georgia (just a sample, you can loop over other states)
scrape_clean_state_results_2024("Georgia")
```

## âœ… 2020 US Presidential Election Results

The function extracts relevant vote counts for Joe Biden and Donald Trump, along with total votes, and standardizes column names across varying table formats. After combining the cleaned datasets, I computed state-level summaries, including total votes for each candidate and their respective vote shares. The summary table and accompanying bar chart reveal differences in voter turnout and candidate support across states, with ***California*** and ***Florida*** contributing the highest total vote counts in the sample.

```{r}
#| code-fold: true
#| code-summary: "Show Code"
#| warning: false
#| message: false

library(xml2)
library(purrr)
library(ggplot2)

# Define scraping + cleaning function
scrape_clean_state_results_2020 <- function(state_name, show_table = FALSE) {
  safe_state <- gsub(" ", "_", state_name)
  url <- paste0("https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_", safe_state)
  html_path <- file.path("data/mp04/html", paste0(safe_state, "_2020.html"))
  dir.create("data/mp04/html", recursive = TRUE, showWarnings = FALSE)

  if (!file.exists(html_path)) {
    resp <- request(url) |> req_perform()
    writeBin(resp_body_raw(resp), html_path)
  }

  page <- read_html(html_path)
  tables <- page |> html_elements("table") |> html_table(fill = TRUE)

  county_table <- NULL
  for (tbl in tables) {
    if (any(str_detect(names(tbl), regex("County|Parish|Borough", ignore_case = TRUE)))) {
      county_table <- tbl
      break
    }
  }

  if (is.null(county_table)) {
    warning(paste("No county-level table found for", state_name))
    return(NULL)
  }

  county_table <- county_table |> clean_names()
  names(county_table) <- tolower(names(county_table))

  if (!any(str_detect(names(county_table), "biden")) | !any(str_detect(names(county_table), "trump"))) {
    warning(paste("Expected vote columns not found in", state_name))
    return(NULL)
  }

  cleaned <- county_table |> 
    select(matches("county|trump|biden|total|votes")) |> 
    mutate(state = state_name) |> 
    mutate(across(where(is.character), ~ gsub(",", "", .))) |> 
    mutate(across(matches("trump|biden|total|votes"), ~ suppressWarnings(as.numeric(.))))

  if (!"total_votes" %in% names(cleaned)) {
    tv_col <- names(cleaned)[str_detect(names(cleaned), "total|votes")][1]
    if (!is.na(tv_col)) {
      cleaned <- cleaned |> rename(total_votes = all_of(tv_col))
    }
  }

  if (show_table) {
    return(kable(head(cleaned, 10), caption = paste("2020 Results for", state_name)))
  } else {
    return(cleaned)
  }
}

# Select 50 states for full data
states <- state.name  # All U.S. states from the built-in state.name dataset

# Scrape and clean data
results_2020 <- map(states, ~ scrape_clean_state_results_2020(.x))
results_2020_df <- bind_rows(results_2020[!sapply(results_2020, is.null)])

# Summarize vote totals by state
state_summary_2020 <- results_2020_df |>
  group_by(state) |>
  summarise(
    total_biden = sum(joe_biden_democratic, na.rm = TRUE),
    total_trump = sum(donald_trump_republican, na.rm = TRUE),
    total_votes = sum(total_votes, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(
    biden_pct = round(100 * total_biden / total_votes, 1),
    trump_pct = round(100 * total_trump / total_votes, 1)
  ) |>
  arrange(desc(total_votes))

# View summary
kable(
  head(state_summary_2020, 10),
  caption = "Top States by Total Votes in the 2020 Presidential Election"
)
```

## Combining all 3 datasets 

```{r}
#| message: false
#| warning: false
#| code-fold: true
#| code-summary: Show Code
#| include: false

```
